{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red164\green191\blue255;\red23\green23\blue23;\red255\green255\blue255;
\red252\green115\blue96;\red117\green255\blue242;\red129\green131\blue134;\red254\green219\blue112;}
{\*\expandedcolortbl;;\cssrgb\c70196\c80000\c100000;\cssrgb\c11765\c11765\c11765;\cssrgb\c100000\c100000\c100000;
\cssrgb\c100000\c53725\c45098;\cssrgb\c51373\c100000\c96078;\cssrgb\c57647\c58431\c59608;\cssrgb\c100000\c87843\c51373;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh15100\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \'97
\f1\fs28\fsmilli14080 \cf2 \cb3 \expnd0\expndtw0\kerning0
import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\cf5 \cb3 months\cf4  = [\cf5 1\cf4 , \cf5 2\cf4 , \cf5 3\cf4 , \cf5 4\cf4 , \cf5 5\cf4 , \cf5 6\cf4 , \cf5 7\cf4 , \cf5 8\cf4 , \cf5 9\cf4 , \cf5 10\cf4 , \cf5 11\cf4 , \cf5 12\cf4 ]\cb1 \
\cf5 \cb3 revenue\cf4  = [\cf5 52\cf4 , \cf5 74\cf4 , \cf5 79\cf4 , \cf5 95\cf4 , \cf5 115\cf4 , \cf5 110\cf4 , \cf5 129\cf4 , \cf5 126\cf4 , \cf5 147\cf4 , \cf5 146\cf4 , \cf5 156\cf4 , \cf5 184\cf4 ]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #slope:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 m\cf4  = \cf5 10\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #intercept:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 b\cf4  = \cf5 50\cf4 \cb1 \
\
\cf5 \cb3 y\cf4 =[\cf5 m\cf4 *\cf5 i\cf4 +\cf5 b\cf4  \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  \cf5 months\cf4 ]\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 months\cf4 ,\cf5 y\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 months\cf4 , \cf5 revenue\cf4 , \cf8 "o"\cf4 )\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 show\cf4 ()\cb1 \
\
\'97\cf5 \cb3 x\cf4  = [\cf5 1\cf4 , \cf5 2\cf4 , \cf5 3\cf4 ]\cb1 \
\cf5 \cb3 y\cf4  = [\cf5 5\cf4 , \cf5 1\cf4 , \cf5 3\cf4 ]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #y = x\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 m1\cf4  = \cf5 1\cf4 \cb1 \
\cf5 \cb3 b1\cf4  = \cf5 0\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #y = 0.5x + 1\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 m2\cf4  = \cf5 0.5\cf4 \cb1 \
\cf5 \cb3 b2\cf4  = \cf5 1\cf4 \cb1 \
\
\cf5 \cb3 y_predicted1\cf4 =[\cf5 1\cf4 ,\cf5 2\cf4 ,\cf5 3\cf4 ]\cb1 \
\cf5 \cb3 y_predicted2\cf4 =[\cf5 1.5\cf4 ,\cf5 2\cf4 ,\cf5 2.5\cf4 ]\cb1 \
\
\cf5 \cb3 total_loss1\cf4 =(\cf5 5-1\cf4 )**\cf5 2\cf4 +(\cf5 1-2\cf4 )**\cf5 2\cf4 +(\cf5 3-3\cf4 )**\cf5 2\cf4 \cb1 \
\cf5 \cb3 total_loss2\cf4 =(\cf5 5-1.5\cf4 )**\cf5 2\cf4 +(\cf5 1-2\cf4 )**\cf5 2\cf4 +(\cf5 3-2.5\cf4 )**\cf5 2\cf4 \cb1 \
\
\cb3 print(\cf5 total_loss1\cf4 )\cb1 \
\cb3 print(\cf5 total_loss2\cf4 )\cb1 \
\
\cf5 \cb3 better_fit\cf4 =\cf5 2\cf4 \cb1 \
\
\'97\cf2 \cb3 def get_gradient_at_b\cf4 (\cf5 x\cf4 ,\cf5 y\cf4 ,\cf5 m\cf4 ,\cf5 b\cf4 ):\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 0\cf4 , len(\cf5 x\cf4 )):\cb1 \
\cb3     \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 diff\cf4  += (\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\
\cb3   \cf5 b_gradient\cf4 =((\cf5 -2\cf4 )/\cf5 N\cf4 )*\cf5 diff\cf4 \cb1 \
\cb3   \cf2 return\cf4  \cf5 b_gradient\cf4 \cb1 \
\
\'97 \cf2 \cb3 def get_gradient_at_m\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 m\cf4 , \cf5 b\cf4 ):\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 N\cf4 ):\cb1 \
\cb3     \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 diff\cf4  += \cf5 x_val\cf4 *(\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\cb3   \cf5 m_gradient\cf4  = \cf5 -2\cf4 /\cf5 N\cf4  * \cf5 diff\cf4 \cb1 \
\cb3   \cf2 return\cf4  \cf5 m_gradient\cf4 \cb1 \
\
\
\'97\cf2 \cb3 def get_gradient_at_b\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b\cf4 , \cf5 m\cf4 ):\cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 N\cf4 ):\cb1 \
\cb3     \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 diff\cf4  += (\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\cb3   \cf5 b_gradient\cf4  = -(\cf5 2\cf4 /\cf5 N\cf4 ) * \cf5 diff\cf4   \cb1 \
\cb3   \cf2 return\cf4  \cf5 b_gradient\cf4 \cb1 \
\
\cf2 \cb3 def get_gradient_at_m\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b\cf4 , \cf5 m\cf4 ):\cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 N\cf4 ):\cb1 \
\cb3       \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3       \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3       \cf5 diff\cf4  += \cf5 x_val\cf4  * (\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\cb3   \cf5 m_gradient\cf4  = -(\cf5 2\cf4 /\cf5 N\cf4 ) * \cf5 diff\cf4   \cb1 \
\cb3   \cf2 return\cf4  \cf5 m_gradient\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Define your step_gradient function here\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 def step_gradient\cf4 (\cf5 x\cf4 ,\cf5 y\cf4 ,\cf5 b_current\cf4 ,\cf5 m_current\cf4 ):\cb1 \
\cb3   \cf5 b_gradient\cf4  = \cf5 get_gradient_at_b\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b_current\cf4 , \cf5 m_current\cf4 )\cb1 \
\cb3   \cf5 m_gradient\cf4  = \cf5 get_gradient_at_m\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b_current\cf4 , \cf5 m_current\cf4 )\cb1 \
\cb3   \cf5 b\cf4 = \cf5 b_current\cf4  - (\cf5 0.01\cf4  * \cf5 b_gradient\cf4 )\cb1 \
\cb3   \cf5 m\cf4 = \cf5 m_current\cf4  - (\cf5 0.01\cf4  * \cf5 m_gradient\cf4 )\cb1 \
\cb3   \cf2 return\cf4  [\cf5 b\cf4 ,\cf5 m\cf4 ]\cb1 \
\
\cf5 \cb3 months\cf4  = [\cf5 1\cf4 , \cf5 2\cf4 , \cf5 3\cf4 , \cf5 4\cf4 , \cf5 5\cf4 , \cf5 6\cf4 , \cf5 7\cf4 , \cf5 8\cf4 , \cf5 9\cf4 , \cf5 10\cf4 , \cf5 11\cf4 , \cf5 12\cf4 ]\cb1 \
\cf5 \cb3 revenue\cf4  = [\cf5 52\cf4 , \cf5 74\cf4 , \cf5 79\cf4 , \cf5 95\cf4 , \cf5 115\cf4 , \cf5 110\cf4 , \cf5 129\cf4 , \cf5 126\cf4 , \cf5 147\cf4 , \cf5 146\cf4 , \cf5 156\cf4 , \cf5 184\cf4 ]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # current intercept guess:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 b\cf4  = \cf5 0\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # current slope guess:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 m\cf4  = \cf5 0\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Call your function here to update b and m\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 b\cf4 , \cf5 m\cf4  = \cf5 step_gradient\cf4 (\cf5 months\cf4 , \cf5 revenue\cf4 , \cf5 b\cf4 , \cf5 m\cf4 )\cb1 \
\cb3 print(\cf5 b\cf4 , \cf5 m\cf4 )\cb1 \
\
\'97 \cf2 \cb3 import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\cf2 \cb3 from\cf4  \cf5 data\cf4  \cf2 import\cf4  \cf5 bs\cf4 , \cf5 bs_000000001\cf4 , \cf5 bs_01\cf4 \cb1 \
\
\cf5 \cb3 iterations\cf4  = range(\cf5 1400\cf4 )\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 iterations\cf4 , \cf5 bs\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 xlabel\cf4 (\cf8 "Iterations"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 ylabel\cf4 (\cf8 "b value"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 show\cf4 ()\cb1 \
\
\cf5 \cb3 num_iterations\cf4 =\cf5 800\cf4 \cb1 \
\cf5 \cb3 convergence_b\cf4 =\cf5 45\
\
\'97 \cf2 import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\
\cf2 \cb3 def get_gradient_at_b\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b\cf4 , \cf5 m\cf4 ):\cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 N\cf4 ):\cb1 \
\cb3     \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3     \cf5 diff\cf4  += (\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\cb3   \cf5 b_gradient\cf4  = -(\cf5 2\cf4 /\cf5 N\cf4 ) * \cf5 diff\cf4   \cb1 \
\cb3   \cf2 return\cf4  \cf5 b_gradient\cf4 \cb1 \
\
\cf2 \cb3 def get_gradient_at_m\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b\cf4 , \cf5 m\cf4 ):\cb1 \
\cb3   \cf5 N\cf4  = len(\cf5 x\cf4 )\cb1 \
\cb3   \cf5 diff\cf4  = \cf5 0\cf4 \cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 N\cf4 ):\cb1 \
\cb3       \cf5 x_val\cf4  = \cf5 x\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3       \cf5 y_val\cf4  = \cf5 y\cf4 [\cf5 i\cf4 ]\cb1 \
\cb3       \cf5 diff\cf4  += \cf5 x_val\cf4  * (\cf5 y_val\cf4  - ((\cf5 m\cf4  * \cf5 x_val\cf4 ) + \cf5 b\cf4 ))\cb1 \
\cb3   \cf5 m_gradient\cf4  = -(\cf5 2\cf4 /\cf5 N\cf4 ) * \cf5 diff\cf4   \cb1 \
\cb3   \cf2 return\cf4  \cf5 m_gradient\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #Your step_gradient function here\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 def step_gradient\cf4 (\cf5 b_current\cf4 , \cf5 m_current\cf4 , \cf5 x\cf4 , \cf5 y\cf4 , \cf5 learning_rate\cf4 ):\cb1 \
\cb3     \cf5 b_gradient\cf4  = \cf5 get_gradient_at_b\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b_current\cf4 , \cf5 m_current\cf4 )\cb1 \
\cb3     \cf5 m_gradient\cf4  = \cf5 get_gradient_at_m\cf4 (\cf5 x\cf4 , \cf5 y\cf4 , \cf5 b_current\cf4 , \cf5 m_current\cf4 )\cb1 \
\cb3     \cf5 b\cf4  = \cf5 b_current\cf4  - (\cf5 learning_rate\cf4  * \cf5 b_gradient\cf4 )\cb1 \
\cb3     \cf5 m\cf4  = \cf5 m_current\cf4  - (\cf5 learning_rate\cf4  * \cf5 m_gradient\cf4 )\cb1 \
\cb3     \cf2 return\cf4  [\cf5 b\cf4 , \cf5 m\cf4 ]\cb1 \
\cb3   \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #Your gradient_descent function here:  \cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 def gradient_descent\cf4 (\cf5 x\cf4 ,\cf5 y\cf4 ,\cf5 learning_rate\cf4 ,\cf5 num_iterations\cf4 ):\cb1 \
\cb3   \cf5 b\cf4 =\cf5 0\cf4 \cb1 \
\cb3   \cf5 m\cf4 =\cf5 0\cf4 \cb1 \
\cb3   \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  range(\cf5 0\cf4 ,\cf5 num_iterations\cf4 ):\cb1 \
\cb3     \cf5 b\cf4 , \cf5 m\cf4 =\cf5 step_gradient\cf4 (\cf5 b\cf4 , \cf5 m\cf4 , \cf5 x\cf4 , \cf5 y\cf4 , \cf5 learning_rate\cf4 )\cb1 \
\cb3   \cf2 return\cf4  [\cf5 b\cf4 ,\cf5 m\cf4 ]\cb1 \
\
\cf5 \cb3 months\cf4  = [\cf5 1\cf4 , \cf5 2\cf4 , \cf5 3\cf4 , \cf5 4\cf4 , \cf5 5\cf4 , \cf5 6\cf4 , \cf5 7\cf4 , \cf5 8\cf4 , \cf5 9\cf4 , \cf5 10\cf4 , \cf5 11\cf4 , \cf5 12\cf4 ]\cb1 \
\cf5 \cb3 revenue\cf4  = [\cf5 52\cf4 , \cf5 74\cf4 , \cf5 79\cf4 , \cf5 95\cf4 , \cf5 115\cf4 , \cf5 110\cf4 , \cf5 129\cf4 , \cf5 126\cf4 , \cf5 147\cf4 , \cf5 146\cf4 , \cf5 156\cf4 , \cf5 184\cf4 ]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #Uncomment the line below to run your gradient_descent function\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 b\cf4 , \cf5 m\cf4  = \cf5 gradient_descent\cf4 (\cf5 months\cf4 , \cf5 revenue\cf4 , \cf5 0.01\cf4 , \cf5 1000\cf4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #Uncomment the lines below to see the line you've settled upon!\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 y\cf4  = [\cf5 m\cf4 *\cf5 x\cf4  + \cf5 b\cf4  \cf2 for\cf4  \cf5 x\cf4  \cf2 in\cf4  \cf5 months\cf4 ]\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 months\cf4 , \cf5 revenue\cf4 , \cf8 "o"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 months\cf4 , \cf5 y\cf4 )\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 show\cf4 ()\cb1 \
\
\'97 \cf2 \cb3 import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\cf2 \cb3 from\cf4  \cf5 gradient_descent_funcs\cf4  \cf2 import\cf4  \cf5 gradient_descent\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 pandas\cf4  \cf2 as\cf4  \cf5 pd\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\
\cf5 \cb3 df\cf4  = \cf5 pd\cf4 .\cf6 read_csv\cf4 (\cf8 "heights.csv"\cf4 )\cb1 \
\
\cf5 \cb3 X\cf4  = \cf5 df\cf4 [\cf8 "height"\cf4 ]\cb1 \
\cf5 \cb3 y\cf4  = \cf5 df\cf4 [\cf8 "weight"\cf4 ]\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 X\cf4 , \cf5 y\cf4 , \cf8 'o'\cf4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 #plot your line here:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 b\cf4 , \cf5 m\cf4 = \cf5 gradient_descent\cf4 (\cf5 X\cf4 ,\cf5 y\cf4 ,\cf5 num_iterations\cf4 =\cf5 1000\cf4 ,\cf5 learning_rate\cf4 =\cf5 0.0001\cf4 )\cb1 \
\cf5 \cb3 y_predictions\cf4 =[\cf5 i\cf4 *\cf5 m\cf4 +\cf5 b\cf4  \cf2 for\cf4  \cf5 i\cf4  \cf2 in\cf4  \cf5 X\cf4 ]\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 X\cf4 ,\cf5 y_predictions\cf4 )\cb1 \
\cf5 \cb3 \
\cf2 import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\cf2 \cb3 from\cf4  \cf5 sklearn\cf4 .\cf6 linear_model\cf4  \cf2 import\cf4  \cf5 LinearRegression\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 numpy\cf4  \cf2 as\cf4  \cf5 np\cf4 \cb1 \
\
\cf5 \cb3 temperature\cf4  = \cf5 np\cf4 .\cf6 array\cf4 (range(\cf5 60\cf4 , \cf5 100\cf4 , \cf5 2\cf4 ))\cb1 \
\cf5 \cb3 temperature\cf4  = \cf5 temperature\cf4 .\cf6 reshape\cf4 (\cf5 -1\cf4 , \cf5 1\cf4 )\cb1 \
\cf5 \cb3 sales\cf4  = [\cf5 65\cf4 , \cf5 58\cf4 , \cf5 46\cf4 , \cf5 45\cf4 , \cf5 44\cf4 , \cf5 42\cf4 , \cf5 40\cf4 , \cf5 40\cf4 , \cf5 36\cf4 , \cf5 38\cf4 , \cf5 38\cf4 , \cf5 28\cf4 , \cf5 30\cf4 , \cf5 22\cf4 , \cf5 27\cf4 , \cf5 25\cf4 , \cf5 25\cf4 , \cf5 20\cf4 , \cf5 15\cf4 , \cf5 5\cf4 ]\cb1 \
\
\cf5 \cb3 line_fitter\cf4  = \cf5 LinearRegression\cf4 ()\cb1 \
\cf5 \cb3 line_fitter\cf4 .\cf6 fit\cf4 (\cf5 temperature\cf4 , \cf5 sales\cf4 )\cb1 \
\
\cf5 \cb3 sales_predict\cf4 = \cf5 line_fitter\cf4 .\cf6 predict\cf4 (\cf5 temperature\cf4 )\
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 plt\cf4 \cb3 .\cf6 \cb3 plot\cf4 \cb3 (\cf5 \cb3 temperature\cf4 \cb3 , \cf5 \cb3 sales\cf4 \cb3 , \cf8 \cb3 'o'\cf4 \cb3 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 temperature\cf4 , \cf5 sales_predict\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 show\cf4 ()\cb1 \
\cf5 \cb3 \
\
\
\'97\cf2 import\cf4  \cf5 codecademylib3_seaborn\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 matplotlib\cf4 .\cf6 pyplot\cf4  \cf2 as\cf4  \cf5 plt\cf4 \cb1 \
\cf2 \cb3 import\cf4  \cf5 pandas\cf4  \cf2 as\cf4  \cf5 pd\cf4 \cb1 \
\cf2 \cb3 from\cf4  \cf5 sklearn\cf4 .\cf6 linear_model\cf4  \cf2 import\cf4  \cf5 LinearRegression\cf4 \cb1 \
\cf2 \cb3 from\cf4  \cf5 sklearn\cf4 .\cf6 datasets\cf4  \cf2 import\cf4  \cf5 load_boston\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Boston housing dataset\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 boston\cf4  = \cf5 load_boston\cf4 ()\cb1 \
\
\cf5 \cb3 df\cf4  = \cf5 pd\cf4 .\cf6 DataFrame\cf4 (\cf5 boston\cf4 .\cf6 data\cf4 , \cf5 columns\cf4  = \cf5 boston\cf4 .\cf6 feature_names\cf4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Set the x-values to the nitrogen oxide concentration:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 X\cf4  = \cf5 df\cf4 [[\cf8 'NOX'\cf4 ]]\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Y-values are the prices:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 y\cf4  = \cf5 boston\cf4 .\cf6 target\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Can we do linear regression on this?\cf4 \cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 line_fitter\cf4  = \cf5 LinearRegression\cf4 ()\cb1 \
\cf5 \cb3 line_fitter\cf4 .\cf6 fit\cf4 (\cf5 X\cf4 ,\cf5 y\cf4 )\cb1 \
\
\cf5 \cb3 y_\cf4 = \cf5 line_fitter\cf4 .\cf6 predict\cf4 (\cf5 X\cf4 )\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 scatter\cf4 (\cf5 X\cf4 , \cf5 y\cf4 , \cf5 alpha\cf4 =\cf5 0.4\cf4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 # Plot line here:\cf4 \cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 plt\cf4 .\cf6 plot\cf4 (\cf5 X\cf4 ,\cf5 y_\cf4 )\cb1 \
\
\cf5 \cb3 plt\cf4 .\cf6 title\cf4 (\cf8 "Boston Housing Dataset"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 xlabel\cf4 (\cf8 "Nitric Oxides Concentration"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 ylabel\cf4 (\cf8 "House Price ($)"\cf4 )\cb1 \
\cf5 \cb3 plt\cf4 .\cf6 show\cf4 ()\cb1 \
\cf5 \cb3 \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\cf4 \cb1 \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
}