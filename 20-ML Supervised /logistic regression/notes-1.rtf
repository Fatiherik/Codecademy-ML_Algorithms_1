{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\fnil\fcharset0 HelveticaNeue;
}
{\colortbl;\red255\green255\blue255;\red164\green191\blue255;\red23\green23\blue23;\red255\green255\blue255;
\red252\green115\blue96;\red117\green255\blue242;\red129\green131\blue134;\red254\green219\blue112;\red56\green56\blue56;
}
{\*\expandedcolortbl;;\cssrgb\c70196\c80000\c100000;\cssrgb\c11765\c11765\c11765;\cssrgb\c100000\c100000\c100000;
\cssrgb\c100000\c53725\c45098;\cssrgb\c51373\c100000\c96078;\cssrgb\c57647\c58431\c59608;\cssrgb\c100000\c87843\c51373;\cssrgb\c28235\c28235\c28235;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}}{\leveltext\leveltemplateid1\'00;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww28600\viewh18000\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \'97 
\f1\fs28\fsmilli14080 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 codecademylib3_seaborn\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 matplotlib\cf4 \strokec4 .\cf6 \strokec6 pyplot\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 plt\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 sklearn\cf4 \strokec4 .\cf6 \strokec6 linear_model\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 LinearRegression\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 hours_studied\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 plotter\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 plot_data\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Create linear regression model\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 model\cf4 \strokec4  = \cf5 \strokec5 LinearRegression\cf4 \strokec4 ()\cb1 \
\cf5 \cb3 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 fit\cf4 \strokec4 (\cf5 \strokec5 hours_studied\cf4 \strokec4 ,\cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Plug sample data into fitted model\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 sample_x\cf4 \strokec4  = \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 linspace\cf4 \strokec4 (\cf5 \strokec5 -16.65\cf4 \strokec4 , \cf5 \strokec5 33.35\cf4 \strokec4 , \cf5 \strokec5 300\cf4 \strokec4 ).\cf6 \strokec6 reshape\cf4 \strokec4 (\cf5 \strokec5 -1\cf4 \strokec4 ,\cf5 \strokec5 1\cf4 \strokec4 )\cb1 \
\cf5 \cb3 \strokec5 probability\cf4 \strokec4  = \cf5 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 predict\cf4 \strokec4 (\cf5 \strokec5 sample_x\cf4 \strokec4 ).\cf6 \strokec6 ravel\cf4 \strokec4 ()\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Function to plot exam data and linear regression curve\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 plot_data\cf4 \strokec4 (\cf5 \strokec5 model\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Show the plot\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 show\cf4 \strokec4 ()\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Define studios and slacker here\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 slacker\cf4 \strokec4 =\cf5 \strokec5 -0.08\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 studious\cf4 \strokec4 =\cf5 \strokec5 1.1\cf4 \cb1 \strokec4 \
\
\'97\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 codecademylib3_seaborn\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 matplotlib\cf4 \strokec4 .\cf6 \strokec6 pyplot\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 plt\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 sklearn\cf4 \strokec4 .\cf6 \strokec6 linear_model\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 LogisticRegression\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 hours_studied\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 plotter\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 plot_data\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Create logistic regression model\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 model\cf4 \strokec4  = \cf5 \strokec5 LogisticRegression\cf4 \strokec4 ()\cb1 \
\cf5 \cb3 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 fit\cf4 \strokec4 (\cf5 \strokec5 hours_studied\cf4 \strokec4 ,\cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Plug sample data into fitted model\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 sample_x\cf4 \strokec4  = \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 linspace\cf4 \strokec4 (\cf5 \strokec5 -16.65\cf4 \strokec4 , \cf5 \strokec5 33.35\cf4 \strokec4 , \cf5 \strokec5 300\cf4 \strokec4 ).\cf6 \strokec6 reshape\cf4 \strokec4 (\cf5 \strokec5 -1\cf4 \strokec4 ,\cf5 \strokec5 1\cf4 \strokec4 )\cb1 \
\cf5 \cb3 \strokec5 probability\cf4 \strokec4  = \cf5 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 predict_proba\cf4 \strokec4 (\cf5 \strokec5 sample_x\cf4 \strokec4 )[:,\cf5 \strokec5 1\cf4 \strokec4 ]\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Function to plot exam data and logistic regression curve\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 plot_data\cf4 \strokec4 (\cf5 \strokec5 model\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Show the plot\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 show\cf4 \strokec4 ()\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf7 \cb3 \strokec7 # Lowest and highest probabilities\cf4 \cb1 \strokec4 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 lowest\cf4 \strokec4 =\cf5 \strokec5 0\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 highest\cf4 \strokec4 =\cf5 \strokec5 1\
\
\'97\cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 hours_studied\cf4 \strokec4 , \cf5 \strokec5 calculated_coefficients\cf4 \strokec4 , \cf5 \strokec5 intercept\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Create your log_odds() function here\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 def log_odds\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 ,\cf5 \strokec5 coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 ):\cb1 \
\cb3   \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 dot\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 , \cf5 \strokec5 coefficients\cf4 \strokec4 ) + \cf5 \strokec5 intercept\cf4 \cb1 \strokec4 \
\
\
\cf7 \cb3 \strokec7 # Calculate the log-odds for the Codecademy University data here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 calculated_log_odds\cf4 \strokec4 =\cf5 \strokec5 log_odds\cf4 \strokec4 (\cf5 \strokec5 hours_studied\cf4 \strokec4 ,\cf5 \strokec5 calculated_coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 calculated_log_odds\cf4 \strokec4 )\
\
\'97\cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 codecademylib3_seaborn\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 calculated_log_odds\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Create your sigmoid function here\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 def sigmoid\cf4 \strokec4 (\cf5 \strokec5 z\cf4 \strokec4 ):\cb1 \
\cb3   \cf5 \strokec5 denominator\cf4 \strokec4 =\cf5 \strokec5 1\cf4 \strokec4 +\cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 exp\cf4 \strokec4 (-\cf5 \strokec5 z\cf4 \strokec4 )\cb1 \
\cb3   \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 1\cf4 \strokec4 /\cf5 \strokec5 denominator\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Calculate the sigmoid of the log-odds here\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 probabilities\cf4 \strokec4 =\cf5 \strokec5 sigmoid\cf4 \strokec4 (\cf5 \strokec5 calculated_log_odds\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 probabilities\cf4 \strokec4 )\cb1 \
\
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 \'97\cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 passed_exam\cf4 \strokec4 , \cf5 \strokec5 probabilities\cf4 \strokec4 , \cf5 \strokec5 probabilities_2\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Function to calculate log-loss\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 def log_loss\cf4 \strokec4 (\cf5 \strokec5 probabilities\cf4 \strokec4 ,\cf5 \strokec5 actual_class\cf4 \strokec4 ):\cb1 \
\cb3   \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 sum\cf4 \strokec4 (-(\cf5 \strokec5 1\cf4 \strokec4 /\cf5 \strokec5 actual_class\cf4 \strokec4 .\cf6 \strokec6 shape\cf4 \strokec4 [\cf5 \strokec5 0\cf4 \strokec4 ])*(\cf5 \strokec5 actual_class\cf4 \strokec4 *\cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 log\cf4 \strokec4 (\cf5 \strokec5 probabilities\cf4 \strokec4 ) + (\cf5 \strokec5 1\cf4 \strokec4 -\cf5 \strokec5 actual_class\cf4 \strokec4 )*\cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 log\cf4 \strokec4 (\cf5 \strokec5 1\cf4 \strokec4 -\cf5 \strokec5 probabilities\cf4 \strokec4 )))\cb1 \
\
\cf7 \cb3 \strokec7 # Print passed_exam here\cf4 \cb1 \strokec4 \
\
\cb3 print(\cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\
\
\cf7 \cb3 \strokec7 # Calculate and print loss_1 here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 loss_1\cf4 \strokec4 =\cf5 \strokec5 log_loss\cf4 \strokec4 (\cf5 \strokec5 probabilities\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 loss_1\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Calculate and print loss_2 here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 loss_2\cf4 \strokec4 =\cf5 \strokec5 log_loss\cf4 \strokec4 (\cf5 \strokec5 probabilities_2\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 loss_2\cf4 \strokec4 )\cb1 \
\
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 hours_studied\cf4 \strokec4 , \cf5 \strokec5 calculated_coefficients\cf4 \strokec4 , \cf5 \strokec5 intercept\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 def log_odds\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 , \cf5 \strokec5 coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 ):\cb1 \
\cb3   \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 dot\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 ,\cf5 \strokec5 coefficients\cf4 \strokec4 ) + \cf5 \strokec5 intercept\cf4 \cb1 \strokec4 \
\
\cf2 \cb3 \strokec2 def sigmoid\cf4 \strokec4 (\cf5 \strokec5 z\cf4 \strokec4 ):\cb1 \
\cb3     \cf5 \strokec5 denominator\cf4 \strokec4  = \cf5 \strokec5 1\cf4 \strokec4  + \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 exp\cf4 \strokec4 (-\cf5 \strokec5 z\cf4 \strokec4 )\cb1 \
\cb3     \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 1\cf4 \strokec4 /\cf5 \strokec5 denominator\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Create predict_class() function here\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 def predict_class\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 ,\cf5 \strokec5 coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 ,\cf5 \strokec5 threshold\cf4 \strokec4 ):\cb1 \
\cb3   \cf5 \strokec5 calculated_log_odds\cf4 \strokec4 = \cf5 \strokec5 log_odds\cf4 \strokec4 (\cf5 \strokec5 features\cf4 \strokec4 , \cf5 \strokec5 coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 )\cb1 \
\cb3   \cf5 \strokec5 probabilities\cf4 \strokec4 =\cf5 \strokec5 sigmoid\cf4 \strokec4 (\cf5 \strokec5 calculated_log_odds\cf4 \strokec4 )\cb1 \
\cb3   \cf2 \strokec2 return\cf4 \strokec4  \cf5 \strokec5 np\cf4 \strokec4 .\cf6 \strokec6 where\cf4 \strokec4 (\cf5 \strokec5 probabilities\cf4 \strokec4  \cf5 \strokec5 >\cf4 \strokec4 = \cf5 \strokec5 threshold\cf4 \strokec4 , \cf5 \strokec5 1\cf4 \strokec4 , \cf5 \strokec5 0\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Make final classifications on Codecademy University data here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 final_results\cf4 \strokec4 =\cf5 \strokec5 predict_class\cf4 \strokec4 (\cf5 \strokec5 hours_studied\cf4 \strokec4 , \cf5 \strokec5 calculated_coefficients\cf4 \strokec4 , \cf5 \strokec5 intercept\cf4 \strokec4 , \cf5 \strokec5 0.5\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 final_results\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 \
\'97\cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 sklearn\cf4 \strokec4 .\cf6 \strokec6 linear_model\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 LogisticRegression\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 hours_studied_scaled\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \strokec4 , \cf5 \strokec5 exam_features_scaled_train\cf4 \strokec4 , \cf5 \strokec5 exam_features_scaled_test\cf4 \strokec4 , \cf5 \strokec5 passed_exam_2_train\cf4 \strokec4 , \cf5 \strokec5 passed_exam_2_test\cf4 \strokec4 , \cf5 \strokec5 guessed_hours_scaled\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Create and fit logistic regression model here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 model\cf4 \strokec4  = \cf5 \strokec5 LogisticRegression\cf4 \strokec4 ()\cb1 \
\cf5 \cb3 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 fit\cf4 \strokec4 (\cf5 \strokec5 hours_studied_scaled\cf4 \strokec4 , \cf5 \strokec5 passed_exam\cf4 \strokec4 )\cb1 \
\cf7 \cb3 \strokec7 # Save the model coefficients and intercept here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 calculated_coefficients\cf4 \strokec4 =\cf5 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 coef_\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 intercept\cf4 \strokec4 =\cf5 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 intercept_\cf4 \cb1 \strokec4 \
\cb3 print(\cf5 \strokec5 calculated_coefficients\cf4 \strokec4 ,\cf5 \strokec5 intercept\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Predict the probabilities of passing for next semester's students here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 passed_predictions\cf4 \strokec4 =\cf5 \strokec5 model\cf4 \strokec4 .\cf6 \strokec6 predict_proba\cf4 \strokec4 (\cf5 \strokec5 guessed_hours_scaled\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 passed_predictions\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Create a new model on the training data with two features here\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 model_2\cf4 \strokec4  = \cf5 \strokec5 LogisticRegression\cf4 \strokec4 ()\cb1 \
\cf5 \cb3 \strokec5 model_2\cf4 \strokec4 .\cf6 \strokec6 fit\cf4 \strokec4 (\cf5 \strokec5 exam_features_scaled_train\cf4 \strokec4 , \cf5 \strokec5 passed_exam_2_train\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Predict whether the students will pass here\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 passed_predictions_2\cf4 \strokec4 =\cf5 \strokec5 model_2\cf4 \strokec4 .\cf6 \strokec6 predict\cf4 \strokec4 (\cf5 \strokec5 exam_features_scaled_test\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 passed_predictions_2\cf4 \strokec4 )\cb1 \
\cb3 print(\cf5 \strokec5 passed_exam_2_test\cf4 \strokec4 )\cb1 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 \
\
\
\
\'97\cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 codecademylib3_seaborn\cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 numpy\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 np\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 matplotlib\cf4 \strokec4 .\cf6 \strokec6 pyplot\cf4 \strokec4  \cf2 \strokec2 as\cf4 \strokec4  \cf5 \strokec5 plt\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 sklearn\cf4 \strokec4 .\cf6 \strokec6 linear_model\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 LogisticRegression\cf4 \cb1 \strokec4 \
\cf2 \cb3 \strokec2 from\cf4 \strokec4  \cf5 \strokec5 exam\cf4 \strokec4  \cf2 \strokec2 import\cf4 \strokec4  \cf5 \strokec5 exam_features_scaled\cf4 \strokec4 , \cf5 \strokec5 passed_exam_2\cf4 \cb1 \strokec4 \
\
\cf7 \cb3 \strokec7 # Train a sklearn logistic regression model on the normalized exam data\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 model_2\cf4 \strokec4  = \cf5 \strokec5 LogisticRegression\cf4 \strokec4 ()\cb1 \
\cf5 \cb3 \strokec5 model_2\cf4 \strokec4 .\cf6 \strokec6 fit\cf4 \strokec4 (\cf5 \strokec5 exam_features_scaled\cf4 \strokec4 ,\cf5 \strokec5 passed_exam_2\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Assign and update coefficients\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 coefficients\cf4 \strokec4 =\cf5 \strokec5 model_2\cf4 \strokec4 .\cf6 \strokec6 coef_\cf4 \cb1 \strokec4 \
\cf5 \cb3 \strokec5 coefficients\cf4 \strokec4 = \cf5 \strokec5 coefficients\cf4 \strokec4 .\cf6 \strokec6 tolist\cf4 \strokec4 ()[\cf5 \strokec5 0\cf4 \strokec4 ]\cb1 \
\cb3 print(\cf5 \strokec5 coefficients\cf4 \strokec4 )\cb1 \
\
\cf7 \cb3 \strokec7 # Plot bar graph\cf4 \cb1 \strokec4 \
\
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 bar\cf4 \strokec4 ([\cf5 \strokec5 1\cf4 \strokec4 ,\cf5 \strokec5 2\cf4 \strokec4 ],\cf5 \strokec5 coefficients\cf4 \strokec4 )\cb1 \
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 xticks\cf4 \strokec4 ([\cf5 \strokec5 1\cf4 \strokec4 ,\cf5 \strokec5 2\cf4 \strokec4 ],[\cf8 \strokec8 'hours studied'\cf4 \strokec4 ,\cf8 \strokec8 'math courses taken'\cf4 \strokec4 ])\cb1 \
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 xlabel\cf4 \strokec4 (\cf8 \strokec8 'feature'\cf4 \strokec4 )\cb1 \
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 ylabel\cf4 \strokec4 (\cf8 \strokec8 'coefficient'\cf4 \strokec4 )\cb1 \
\
\cf5 \cb3 \strokec5 plt\cf4 \strokec4 .\cf6 \strokec6 show\cf4 \strokec4 ()\
\
\'97
\f2\fs35\fsmilli17600 \cf9 \cb4 \strokec9 Since our data is normalized, all features vary over the same range. Given this understanding, we can compare the feature coefficients\'92 magnitudes and signs to determine which features have the greatest impact on class prediction, and if that impact is positive or negative.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl560\sa120\partightenfactor0
\ls1\ilvl0\cf9 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 		\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec9 Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class\cb1 \
\ls1\ilvl0\cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 		\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec9 Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class\cb1 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl560\sa240\partightenfactor0
\ls1\ilvl0\cf9 \cb4 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 		\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec9 Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class\cb1 \
\pard\pardeftab720\sl560\partightenfactor0
\cf9 \cb4 Given cancer data, a logistic regression model can let us know what features are most important for predicting survival after, for example, five years from diagnosis. Knowing these features can lead to a better understanding of outcomes, and even lives saved!\
\pard\pardeftab720\sl440\partightenfactor0

\f1\fs28\fsmilli14080 \cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\partightenfactor0
\cf5 \cb3 \strokec5 \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\pard\pardeftab720\sl440\partightenfactor0
\cf4 \cb1 \strokec4 \
}