
1) supervised:
	+Regression:
		- Linear regression
		- K-nearest neighbors (KNN) regression

	+Classification:
		- Naive Bayes
		- K- nearest neighbors (KNN) 
		- Logistic regression
		- Support Vektor Machines (SVM)
		- Decision Trees
		- Random Forests

2) unsupervised:

	+ Cluster
		- K-means




— Linear Regression
Model hata kareler toplamını (HKT) minimum yapan b0 ve m katsayilarini bulup y=b0+mx seklinde bir denklem kuruyor. 
Modeli fit ettiginde hkt de onemli bir degisiklik olmayincaya kadar b0 ve m değişkenlerini sürekli deneyerek optimum b0 ve m yi buluyor.
Import and create the model:
from sklearn.linear_model import LinearRegression

your_model = LinearRegression()
Fit:
your_model.fit(x_training_data, y_training_data)
		.coef_: contains the coefficients
		.intercept_: contains the intercept
Predict:
predictions = your_model.predict(your_x_data)
		.score(): returns the coefficient of determination R²
Ornek-1:
regr = linear_model.LinearRegression()
regr.fit(X,y)
print(regr.coef_, regr.intercept_)

y_predict= regr.predict(X)

plt.scatter(X,y)
plt.plot(X,y_predict)


X_future = np.array(range(2013, 2050))
X_future=X_future.reshape(-1,1)

future_predict= regr.predict(X_future)
print(future_predict)
plt.plot(X_future,future_predict)
plt.show()

Ornek-2:
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

streeteasy = pd.read_csv("https://raw.githubusercontent.com/sonnynomnom/Codecademy-Machine-Learning-Fundamentals/master/StreetEasy/manhattan.csv")

df = pd.DataFrame(streeteasy)

x = df[['bedrooms', 'bathrooms', 'size_sqft', 'min_to_subway', 'floor', 'building_age_yrs', 'no_fee', 'has_roofdeck', 'has_washer_dryer', 'has_doorman', 'has_elevator', 'has_dishwasher', 'has_patio', 'has_gym']]

y = df[['rent']]

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state=6)

mlr = LinearRegression()

model=mlr.fit(x_train, y_train)

y_predict = mlr.predict(x_test)

# Input code here:

# train R2 si

print("Train score:")
print(mlr.score(x_train, y_train))

# test R2 si

print("Test score:")
print(mlr.score(x_test, y_test))

residuals = y_predict - y_test

plt.scatter(y_predict, residuals, alpha=0.4)
plt.title('Residual Analysis')

plt.show()


Naive Bayes
Import and create the model:
from sklearn.naive_bayes import MultinomialNB

your_model = MultinomialNB()
Fit:
your_model.fit(x_training_data, y_training_data)
Predict:
# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data)

Esasen P(A ∣ B)=(P(B ∣ A)⋅P(A))/P(B) bu formülün sonucunu bulmaya calisiyoruz. Formül B bilindiğinde A nin olma olasiligi demek. B nin yerine data point, A nin yerine de class koyarsak bir model oluyor. Ornegin bir e-mailin spam olup olmadigini bulmaya calisiyoruz. Spam olma olasiligi P(spam | email) ile  spam olmama olasiligini P(not spam | email) karsilastiririz. Hangi olasılık daha büyükse onu seceriz.
Naive Bayes genelde text classification icin kullanılıyor. Ornegin elimizde review= “This crib was amazing” diye bir yorum olsun ve bu yorumun olumlu mu yoksa olumsuz mu Oldugunu öğrenmek istiyoruz.  P(positive | review) ve P(negative | review) olasiliklarini karsilastirmamiz lazım. Elimizde yorumlar ve bu yorumların olumlu/olumsuz oldugunu gösteren labellar var.

Yorumun olumlu olma ihtimali P(positive | review)= (P(review | positive)*P(positive))/P(review)
P(positive)= herhangi bir yorumun pozitif olma ihtimali( pozitif yorum sayısı /tum yorum sayısı)
P(review | positive)= yorumumuz icindeki kelimelerin (“This”, “crib”, “was”, and “amazing”) pozitif yorumlar icinde olma olasiligi demek.
	P(“This crib was amazing" ∣ positive)=P(“This" ∣ positive)*P(“crib" ∣ positive)*P(“was" ∣ positive)*P(“amazing" ∣ positive)
		P(“crib" ∣ positive)=(# of “crib" in positive)/(# of words in positive)
P(review)= yorumumuz icindeki kelimelerin (“This”, “crib”, “was”, and “amazing”) tum yorumlar icinde olma olasiligi demek. Yukaridaki gibi hesaplanıyor. P(positive | review) ve P(negative | review) olasiliklarini karsilastirdigimiz icin ve ikisinin de paydası ayni olduğu icin P(review) hesaplamaya gerek yok.

Yorumun olumsuz olma ihtimali P(negative | review)= (P(review | negative)*P(negative))/P(review) yukaridaki gibi hesaplanıyor.
Modeli fit ettiginde arka planda asagidaki gibi bir fonksiyon hazırlıyor. Daha sonra test icin gireceğin yorum bu fonksiyonun bir parametresi oluyor.
Bunu yapan kod:
from reviews import neg_counter, pos_counter

review = "This is super bad"

percent_pos = 0.5
percent_neg = 0.5

total_pos = sum(pos_counter.values())
total_neg = sum(neg_counter.values())

pos_probability = 1
neg_probability = 1

review_words = review.split()

for word in review_words:
  word_in_pos = pos_counter[word]
  word_in_neg = neg_counter[word]
  
  pos_probability *= (word_in_pos + 1) / (total_pos + len(pos_counter))				bir kelime datada hiç yoksa probability formulu sonucu sıfır çıkar, bu olmasın diye Smooting yapılıyor. paydaki +1 ile paydadaki len(…) ordan geliyor.
  neg_probability *= (word_in_neg + 1) / (total_neg + len(neg_counter))

final_pos= pos_probability*percent_pos
final_neg= neg_probability*percent_neg

print(final_pos)
print(final_neg)

if final_pos>final_neg:
  print("The review is positive")
else:
  print("The review is negative")


K-Nearest Neighbors
Import and create the model:
from sklearn.neigbors import KNeighborsClassifier

your_model = KNeighborsClassifier()
Fit:
your_model.fit(x_training_data, y_training_data)
Predict:
# Returns a list of predicted classes - one prediction for every data point
predictions = your_model.predict(your_x_data)

# For every data point, returns a list of probabilities of each class
probabilities = your_model.predict_proba(your_x_data)

Arka planda yapılan şey:
Ornegin elimizde filmlerin isimleri, bu filmlerin ozellikleri (butce, çekildiği yıl, suresi) ve kategorik olarak iyi film (1-0) olup olmadıkları bilgileri var. Bu bilgileri kullanarak listemizde olmayan bir filmin iyi bir film olup olmadigini bulmak istiyoruz. Öncelikle filmin özelliklerini normalize ediyor.(çünkü mesele bütçe 253 milyon yil 2015 bu sayalar ayni degerlendirilmemeli) Sonra Öklid hesabıyla her filmin diğer filme uzakligini buluyor, bu uzaklıkları küçükten büyüğe sıralıyor. Sonra belirlediğimiz k degerine gore en yakin k komsuyu alıyor. K komsu filmin iyi film olup olmadigina bakıyor, eğer sayıca iyi filmler fazlaysa incelediğimiz film iyi sayılıyor ya da vice versa.
Modeli fit ettiginde yukarıdakini test datasına uygulayacak fonksiyonu hazırlıyor.

Validation accuracy: elimizdeki data setini training ve validation set olarak 2 ye ayiriyor, validation setteki her filmi yukaridaki mantıkla 1 ve 0 olarak belirliyor. Sonra bu degerleri filmlerin gercek degerleriyle karsilastirip bir dogruluk degeri buluyor. K degeri değiştikçe doğruluk degeri değişiyor.
When k is small, overfitting occurs and the accuracy is relatively low. On the other hand, when k gets too large, underfitting occurs and accuracy starts to drop.

Ornek-1
rom movies import movie_dataset, labels
from sklearn.neighbors import KNeighborsClassifier

classifier = KNeighborsClassifier(n_neighbors = 5)
classifier.fit(movie_dataset, labels)

guess=classifier.predict([[.45, .2, .5], [.25, .8, .9],[.1, .1, .9]])
print(guess)


Ornek-2 (grafik ile en uygun k  yi bulma)

import codecademylib3_seaborn
from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

breast_cancer_data=load_breast_cancer()

training_data, validation_data, training_labels, validation_labels = train_test_split(breast_cancer_data.data, breast_cancer_data.target, train_size=0.8, test_size=0.2, random_state=100)

accuracies=[]
k_list=range(1,101)

for k in range(1,101):
  classifier = KNeighborsClassifier(n_neighbors = k)
  classifier.fit(training_data,training_labels)

  accuracies.append(classifier.score(validation_data,validation_labels))

plt.plot(k_list,accuracies)
plt.xlabel('k')
plt.ylabel('Validation Accuracy')
plt.title('Breast Cancer Classifier Accuracy')
plt.show()

K-Nearest Neighbors Regression
K-Nearset Neighbors ile ayni yolu izliyor, ancak burada filmi iyi veya kotu olarak siniflandirmaktan ziyade filme puan veriyor. Puanlamayi K komsu filmin puanlarinin ortalamasıni (simple ya da weighted) alarak yapıyor.
Modeli fit ettiginde test datasına uygulanacak fonksiyonu hazırlıyor.

from movies import movie_dataset, movie_ratings
from sklearn.neighbors import KNeighborsRegressor

regressor = KNeighborsRegressor(n_neighbors = 5, weights = "distance”)		 weights=distance ise, distance gore agirlikli ortalamasini alıyor, distance= uniform ise eşit agirliklandirma yapıyor

regressor.fit(movie_dataset, movie_ratings)

guesses = regressor.predict([[0.016, 0.300, 1.022], [0.0004092981, 0.283, 1.0112],
[0.00687649, 0.235, 1.0112]])

print(guesses)

— Logistic Regression

Ikili siniflandirma yapmak icin kullanılır. Öğrenci sınavı gececek mi(1) gecemeyecek mi(0)?
Logistic regression loss fonksiyonu (bir ML modelinin tahmin basarisini ölçer) log loss tur. Model log loss minimum yapan katsayıları bulur, sonra  bu katsayılara gore her öğrencinin sınavı geçme olasiligini hesaplar, olasılık esik degerden (default degeri 0.5) büyükse 1, küçükse 0 degeri verir.

The coefficients determined by a Logistic Regression model can be used to interpret the relative importance of each feature in predicting the class of a data sample.

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(hours_studied_scaled, passed_exam)

calculated_coefficients=model.coef_
intercept=model.intercept_

passed_predictions=model.predict_proba(guessed_hours_scaled)
passed_predictions_2=model_2.predict(exam_features_scaled_test)


Ornek-1:

import numpy as np
from sklearn.linear_model import LogisticRegression
from exam import hours_studied_scaled, passed_exam, exam_features_scaled_train, exam_features_scaled_test, passed_exam_2_train, passed_exam_2_test, guessed_hours_scaled

# Create and fit logistic regression model here
model = LogisticRegression()
model.fit(hours_studied_scaled, passed_exam)
# Save the model coefficients and intercept here
calculated_coefficients=model.coef_
intercept=model.intercept_
print(calculated_coefficients,intercept)

# Predict the probabilities of passing for next semester's students here
passed_predictions=model.predict_proba(guessed_hours_scaled)
print(passed_predictions)

# Create a new model on the training data with two features here
model_2 = LogisticRegression()
model_2.fit(exam_features_scaled_train, passed_exam_2_train)

# Predict whether the students will pass here

passed_predictions_2=model_2.predict(exam_features_scaled_test)
print(passed_predictions_2)
print(passed_exam_2_test)

Ornek-2

import codecademylib3_seaborn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the passenger data
passengers=pd.read_csv('passengers.csv')


# Update sex column to numerical
passengers['Sex']=passengers['Sex'].map({'male':1,'female':2})
# Fill the nan values in the age column

passengers['Age']=passengers['Age'].fillna(passengers['Age'].mean()) 

# Create a first class column
passengers['FirstClass'] = passengers['Pclass'].apply(lambda x: 1 if x == 1 else 0)


# Create a second class column
passengers['SecondClass'] = passengers['Pclass'].apply(lambda x: 1 if x == 2 else 0)

# Select the desired features
features=passengers[['Sex','Age','FirstClass','SecondClass']]
survival=passengers['Survived']

# Perform train, test, split
x_train, x_test, y_train, y_test = train_test_split(features, survival, train_size=0.8, test_size=0.2)

# Scale the feature data so it has mean = 0 and standard deviation = 1

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
# Create and train the model

model = LogisticRegression()
model.fit(x_train, y_train)


# Score the model on the train data

print(model.score(x_train,y_train))
# Score the model on the test data
print(model.score(x_test,y_test))

# Analyze the coefficients
print(model.coef_)
#[[ 1.21532723 -0.39276653  0.93851664  0.46418247]] most important feature is 'Sex'

# Sample passenger features
Jack = np.array([0.0,20.0,0.0,0.0])
Rose = np.array([1.0,17.0,1.0,0.0])
Emma = np.array([1.0,23.0,0.0,1.0])

# Combine passenger arrays
combined_arrays = np.array([ Jack, Rose, Emma ])

# Scale the sample passenger features
combined_arrays= scaler.transform(combined_arrays)
#print(combined_arrays)

# Make survival predictions!
print(model.predict(combined_arrays))
print(model.predict_proba(combined_arrays))


Support Vector Machines(SVM):
Amac noktalar arasından bir decision boundary çizerek siniflandirma yapmak. Veriyi ikiye ayırmak icin verilerin arasindan sonsuz boundary çizebiliriz. SVM en uygun boundary su sekilde buluyor; her sinifin olası boundary çizgisine en yakin noktalarından gececek sekilde support vektörleri çiziyor. Support vektör ile boundary çizgisinin arasındaki mesafeyi (margin) maksimize eden çizgiyi boundary çizgisi olarak belirliyor.
		The C parameter controls how much error is allowed. A large C allows for little error and creates a hard margin(kisa mesafe). A small C allows for more error and creates a soft margin( uzun mesafe)
		SVMs use kernels to classify points that aren’t linearly separable.
		Kernels transform points into higher dimensional space. A polynomial kernel transforms points into three dimensions while an rbf kernel transforms points into infinite dimensions.
		An rbf kernel has a gamma parameter. If gamma is large, the training data is more relevant, and as a result overfitting can occur.
kernel=‘linear’, ‘poly’ ve ‘rlf’ olabiliyor (default deger rbf). Poly: veriyi linear bir çizgi veya linear bir duzlem ile ayiramadigin zaman deneyebilirsin. Bu ucunu deneyerek en yüksek score bul.
Score yükseltmek icin gamma ve c parametrelerini değiştir.
Ornek-1:

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

training_data, validation_data, training_labels, validation_labels = train_test_split(points, labels, train_size = 0.8, test_size = 0.2, random_state = 100)

classifier = SVC(kernel = "rbf", gamma = 0.1)
classifier.fit(training_data,training_labels)

print(classifier.score(validation_data,validation_labels))


Decision Trees 
Siniflandirma yapmak icin kullanılıyor. Amac information gain maksimum yapan ağacı oluşturmak. veriyi dallandiracagiz ama hangi özelliklere gore dallandiracagiz? Ilk fiyatı mi kullanacağız yoksa kapı sayisini mi? Su sekilde yapıyor:
Ornegin elimizde araçlarin özelliklerini (fiyat(çok pahalı=1, pahali=2, gibi), kapı sayısı, benzin tüketimi vs) iceren bir liste (car_data) ile bu araçları alanların araçtan ne düzeyde memnun olduklarını gösteren label lar (cok iyi, iyi, orta vs - car_label) olsun. Model once split edilmemiş ilk verinin gini sayisini buluyor. (Elimizde 3 çok iyi, 4 iyi, 2 orta, 5 kotu, 3 çok kotu varsa; gini=1-(3/17)**2-(4/17)**2 …). Sonra birinci özelliğe (fiyat) gore split ediyor. split edilmiş dalların da ayni hesapla gini sayisini buluyor. Sonra split edilmemiş datanın gini sayısından dalların gini sayilarini çıkararak bir information gain sayısı elde ediyor. Bu sayı eger veriyi fiyata gore dallandirirsak olusacak information gain. Bu sekilde kac özellik varsa ayrı ayrı information gain buluyor. Amac bu information gain sayisini maksimize edecek özelliğe( fiyat, kapı sayısı vs.)  gore dallandirmayi yapmak (Çünkü ilk asamada veriyi hangi özelliğe gore dallandiracagimiza karar veremiyoruz) sonra ayni islemi herbir alt dal icin yapıyor. Dallandıma islemi bir dalın gini sayısı 0 oluncaya kadar (yani bir dalda sadece bir kümeye ait veri kalıncaya kadar) devam ediyor.
— Decision tree ne kadar büyürse (dallanırsa) o kadar train datasına ait bir ağaç oluşturduğun anlamına gelir yani genellikten uzaklasirsin- overfit olursun. accuracy score yükseltmek icin budama yapmak gerekebilir. Ne kadar dallanacagini max_depth ile belirleyebiliyorsun. classifier = DecisionTreeClassifier(random_state = 0, max_depth=11)
from sklearn.tree import DecisionTreeClassifier

classifier = DecisionTreeClassifier(random_state = 0, max_depth=11)	
classifier.fit(training_points, training_labels)
classifier.predict(testing_points)
print(classifier.score(testing_points, testing_labels))

Ornek-1

import codecademylib3_seaborn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

flags=pd.read_csv('flags.csv', header=0)
labels=flags[['Landmass']]
data= flags[["Red", "Green", "Blue", "Gold",
 "White", "Black", "Orange",
 "Circles",
"Crosses","Saltires","Quarters","Sunstars",
"Crescent","Triangle"]]

train_data, test_data, train_labels, test_labels = train_test_split(data,labels,random_state=1)
scores=[]
for i in range(1,21):
  tree= DecisionTreeClassifier(random_state=1, max_depth=i)
  tree.fit(train_data, train_labels)
  scores.append(tree.score(test_data, test_labels))

plt.plot(range(1,21),scores)
plt.show()

Random Forest

Decision trees yapisi gereği overfite egilimli (cunku bir kümenin icinde baska bir class kalmayıncaya kadar dallanmaya devam ediyor). Bu drawback i budama yaparak bir nebze engelleyebiliyoruz ama yeterli degil, bu yüzden random forest kullanıyoruz. Random forest icindeki her karar ağacı bir nokta ile ilgili bir karar veriyor ve en fazla secilen karar random forestin nihai kararı oluyor. Random forest icindeki karar agaclari nasıl oluşturuluyor? 1000 satirlik bir veriden random satir seçerek (secilen satir tekrar secilebilir kosuluyla) yeni bir 1000 satirlik veri oluşturuluyor. Her satir birden fazla secilebildigi icin ve secim random olduğu icin her seferinde farklı bir subtree oluşturuyor. Buna bagging deniyor. bir de feature bagging ile karar agaclarini çeşitlendirme yolu var, o da söyle oluyor: decision trees yönteminde her asamada agaclandirmak icin en uygun özelliği bulurken tum datadaki tum özellikleri hesaba katıyorduk, bunda ise ornegin 20 özellik varsa random 5 ine gore sec diyoruz. Bu da random forest icinde farklı farkli subtree olmasını sağlıyor.

Ornek-1:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier

income_data=pd.read_csv('income.csv', delimiter=', ', header=0)
#print(income_data.iloc[0])

income_data["sex-int"] = income_data["sex"].apply(lambda row: 0 if row == "Male" else 1)
#print(income_data["native-country"].value_counts())
income_data["country-int"] = income_data["native-country"].apply(lambda row: 0 if row == "United-States" else 1)

labels=income_data[['income']]
data=income_data[["age", "capital-gain", "capital-loss", "hours-per-week",'sex-int','country-int']]

train_data, test_data, train_labels, test_labels=train_test_split(data,labels,random_state=1)

forest=RandomForestClassifier(random_state=1)
forest.fit(train_data,train_labels)
print(forest.predict(test_data))
print(forest.feature_importances_)			datadaki sütunların  onem derecelerini veriyor
print(forest.score(test_data,test_labels))
(

K-Means
Import and create the model:
from sklearn.cluster import KMeans

your_model = KMeans(n_clusters=4, init='random')
		n_clusters: number of clusters to form and number of centroids to generate
		init: method for initialization
		k-means++: K-Means++ [default]
		random: K-Means
		random_state: the seed used by the random number generator [optional]
Fit:
your_model.fit(x_training_data)
Predict:
predictions = your_model.predict(your_x_data)

print(classifier.cluster_centers_)

Unsupervised yöntemlerinden birisi. Elimizde labellar yok. datadaki noktalari bir küme merkezine yakinliklarina gore kumelememizi sagliyor. Su sekilde calisiyor: once dataya kac kümeye ayiracagimiza karar veriyoruz (mesela 3). Sonra random olarak 3 tane merkez noktası seciyoruz. Sonra her noktanın bu 3 merkeze uzakligini buluyoruz. her noktayı en yakin merkeze gore labelliyoruz (0,1,2 seklinde). Sonra merkezi 0 olan noktaların mean alarak baska bir nokta daha buluyoruz. Son bulduğumuz mean noktası ile ilk bulduğumuz 0 merkezi örtusene kadar 0 merkezini random belirleme işlemine devam ediyoruz. Ayni islemi 1 merkezi ve 2 merkezi icin de yapiyoruz. Burada iki amac var, birincisi iki noktanın örtüşmesi ikincisi ise örtüşen merkez noktası ile o kümeye ait noktalar arasındaki mesafenin en kısa olması. Burdaki drawback su, random olarak bulunan ilk merkez noktası ile mean noktası ortusebilir ama mesafe acısından en yakin nokta olmayabilir. Bir dikdörtgen dusun, dikdörtgenin hem boyunun hem de eninin orta noktası mean ile örtüşüyor ancak uzaklik acisindan eninin orta noktasını secmek daha basarili. merkez noktalarını Random olarak secmek K-means modeli oluyor, bunun icin parametre olarak init=‘random’ demen lazım, merkez noktası örtüşse bile uzaklık acısından daha yakin noktalari bulmaya calismasi K-means++ modeli oluyor, bunun icinde init=‘k=means++’ ya da hicbisey dememen lazım, çünkü default k-means++. 
En optimum merkez sayisini inertia’ya (her datanın kendi merkezine uzakligi) bakarak bulabilirsin. Kodu:  print(model.inertia_). Daha az inertia daha uygun bir model demek. Data sayısı kadar merkez olması inertia’yi sifir yapar ama bu sefer de kümeleme yapamazsın, modelin yapmaya calistigi, en az merkez sayısı ile en az inertia’yi bulmak.  
Merkez sayısı artisinin inertia’ya marjinal etkisinin azaldigi nokta (grafigin yataylasmaya basladigi nokta) en uygun merkez sayisini verir. Grafiği çizmekteyiz icin asagidaki kodu kullanabilirsin.

iris = datasets.load_iris()

samples = iris.data

# Code Start here:
num_clusters=[1,2,3,4,5,6,7,8]
inertias=[]

for k in num_clusters:
  model = KMeans(n_clusters=k)
  model.fit(samples)
  inertias.append(model.inertia_)
  print(model.inertia_)
plt.plot(num_clusters, inertias, '-o')

plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')

plt.show()


Ornek-1:

from sklearn.cluster import KMeans

iris = datasets.load_iris()

samples = iris.data

model = KMeans(n_clusters=3)

model.fit(samples)

labels = model.predict(samples)

print(labels)

Validating the Model
Import and print accuracy, recall, precision, and F1 score:
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

print(accuracy_score(true_labels, guesses))
print(recall_score(true_labels, guesses))
print(precision_score(true_labels, guesses))
print(f1_score(true_labels, guesses))
Import and print the confusion matrix:
from sklearn.metrics import confusion_matrix

print(confusion_matrix(true_labels, guesses))

Training Sets and Test Sets 
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, test_size=0.2)
		train_size: the proportion of the dataset to include in the train split
		test_size: the proportion of the dataset to include in the test split
		random_state: the seed used by the random number generator [optional]

Perceptrons ve neural aglar
Binary classification problemleri çözmek icin kullanılıyor. Nöronların karar verme süreçlerini taklit ediyor. Elimizde inputlar, bu inputlarin agirliklari ve labellar var. 
weighted_sum=w1*x1+w2*x2
Once weighted_sum hesaplaniyor. weighted_sum sıfırdan büyükse +1, küçükse -1 degeri veriyor, sonra gercek label degerine bakarak error (training error) hesaplıyor. trainingerror = actuallabel − predicted
Inputtaki her deger icin bunu yapıyor ve total error buluyor, sonra training error sıfır oluncaya kadar yani tum degerleri doğru siniflandirana kadar bu agirliklari değiştiriyor (weight = weight + (error ∗ input)  formulune gore ). 
Buldugu agirliklar x lerin katsayıları olduğu icin bunları kullanarak egim hesaplayıp bir decision boundary çizgisi çizerek classification (linear classifier) yapıyor.
** Perceptrons can’t solve problems that aren’t linearly separable. However, if you combine multiple perceptrons together, you now have a neural net that can solve these problems!
from sklearn.linear_model import Perceptron
classifier=Perceptron(max_iter=40)
classifier.fit(data,labels)

Label encoder: sadece 2 gruba dönüştüreceksek kullanılır. Erkek 1 kadın 0 gibi.
One-hot encoder: 2 den fazla gruba ayıracaksak kullanılır.
Nan degerleri grafiklestirme: msno.matrix
